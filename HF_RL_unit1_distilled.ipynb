{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/migolan/RL-notebooks/blob/main/HF_RL_unit1_distilled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on https://huggingface.co/learn/deep-rl-course.\n",
        "* gymnasium for creating the lunar lander environment\n",
        "* stable_baselines3 for creating a PPO RL agent\n",
        "* HF upload/download\n",
        "\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ],
      "metadata": {
        "id": "YWE_n_MUTMYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "7z5ptC3DGYhO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQIGLPDkGhgG"
      },
      "outputs": [],
      "source": [
        "!apt install swig cmake\n",
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip install --upgrade ipykernel\n",
        "!pip install shimmy # conversion tool that will help us run the environment correctly https://github.com/Farama-Foundation/Shimmy\n",
        "import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "J-SOU0VdGc3d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "outputs": [],
      "source": [
        "# RL environments library\n",
        "import gymnasium as gym\n",
        "\n",
        "# RL agents library https://stable-baselines3.readthedocs.io/en/master/\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub # upload and download trained models from the hub\n",
        "# Deep reinforcement Learning models available are listed at https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
        "\n",
        "# for visualization\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the LunarLander-v2 environment"
      ],
      "metadata": {
        "id": "yAXkRebv9N6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create environment"
      ],
      "metadata": {
        "id": "dUtL7c_gKFe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"LunarLander-v2\"  # https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "env = gym.make(env_id)\n",
        "observation, info = env.reset()"
      ],
      "metadata": {
        "id": "QgiusFgJKI37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "State and action spaces"
      ],
      "metadata": {
        "id": "l37PsjSDDqcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation Space Shape:\", env.observation_space.shape)\n",
        "print(\"Observation Space Sample:\", env.observation_space.sample()) # Get a random observation\n",
        "print(\"Action Space Shape:\", env.action_space.n)\n",
        "print(\"Action Space Sample:\", env.action_space.sample()) # sample a random action"
      ],
      "metadata": {
        "id": "FpGPOhtx9cYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment rollout"
      ],
      "metadata": {
        "id": "R_IsuaCnES8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7vOFlpA_ONz"
      },
      "outputs": [],
      "source": [
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "  # sample a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # perform action and observe state and reward\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "  # https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
        "  print(f\"Observation: {observation}\")\n",
        "  print(f\"Reward: {reward}\")\n",
        "  if info:\n",
        "    print(f\"Info: {info}\")\n",
        "\n",
        "  if terminated or truncated:\n",
        "      print(\"Environment is reset\")\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "**Lunar lander envirnoment**\n",
        "\n",
        "**Observation** is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land (boolean)\n",
        "- If the right leg contact point has touched the land (boolean)\n",
        "\n",
        "**The action space** (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n",
        "\n",
        "- Action 0: Do nothing,\n",
        "- Action 1: Fire left orientation engine,\n",
        "- Action 2: Fire the main engine,\n",
        "- Action 3: Fire right orientation engine.\n",
        "\n",
        "**Reward function** (the function that will give a reward at each timestep) üí∞:\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "-  Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
        "\n",
        "An episode is **considered a solution if it scores at least 200 points.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train an RL agent on the environment"
      ],
      "metadata": {
        "id": "pVzj4l9h9nuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vectorized environment - stack multiple independent environments into\n",
        "# a single environment, to have more diverse experiences during the training.\n",
        "env = make_vec_env(env_id, n_envs=16)\n",
        "\n",
        "# Generate an agent with PPO learning algorithm\n",
        "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
        "# if we had frames as input we would use CnnPolicy\n",
        "model = PPO(\n",
        "    policy='MlpPolicy',\n",
        "    env=env,\n",
        "    n_steps=1024,\n",
        "    batch_size=64,\n",
        "    n_epochs=4,\n",
        "    gamma=0.999,\n",
        "    gae_lambda=0.98,\n",
        "    ent_coef=0.01,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "6PHscs1UCfwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will take time, do it on a GPU:"
      ],
      "metadata": {
        "id": "D4kr7ICMCi7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99hqQ_etEy1N"
      },
      "outputs": [],
      "source": [
        "# train PPO agent\n",
        "model.learn(total_timesteps=int(1e6))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ppo-LunarLander-v2\" # A good name is {model_architecture}-{env_id}\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "zCYkpOaOCaeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0n2CfEh-IDM"
      },
      "source": [
        "# Evaluate the agent\n",
        "When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRpno0glsADy"
      },
      "outputs": [],
      "source": [
        "# Create a new environment for evaluation, with a monitor\n",
        "eval_env = Monitor(gym.make(env_id, render_mode='rgb_array'))\n",
        "\n",
        "# Evaluate the model\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push a model to the HF hub"
      ],
      "metadata": {
        "id": "xuxm5Nj0AWmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZiFBBlzxzxY"
      },
      "outputs": [],
      "source": [
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2E--IJu8JYq"
      },
      "outputs": [],
      "source": [
        "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
        "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "\n",
        "package_to_hub(model=model,\n",
        "               model_name=model_name,\n",
        "               model_architecture=\"PPO\",\n",
        "               env_id=env_id,\n",
        "               eval_env=eval_env,\n",
        "               repo_id=f\"migolan/{model_name}\", # A good name is {username}/{model_architecture}-{env_id}\n",
        "               commit_message=\"Upload PPO LunarLander-v2 trained agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T79AEAWEFIxz"
      },
      "source": [
        "The script above should have displayed a link to a model repository such as https://huggingface.co/osanseviero/test_sb3. When you go to this link, you can:\n",
        "* See a video preview of your agent at the right.\n",
        "* Click \"Files and versions\" to see all the files in the repository.\n",
        "* Click \"Use in stable-baselines3\" to get a code snippet that shows how to load the model.\n",
        "* A model card (`README.md` file) which gives a description of the model\n",
        "\n",
        "Compare the results of your LunarLander-v2 with your classmates using the leaderboard üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nWnuQHRfFRa"
      },
      "source": [
        "# Load a saved model from the Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj8PSGHJfwz3"
      },
      "outputs": [],
      "source": [
        "# id this doesn't work amke sure you've installed shimmy\n",
        "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
        "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
        "# Go to https://huggingface.co/models?library=stable-baselines3 to see the list of all the Stable-baselines3 saved models.\n",
        "checkpoint = load_from_hub(repo_id, filename)\n",
        "\n",
        "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
        "# But Python 3.6, 3.7 use protocol 4\n",
        "# In order to get compatibility we need to:\n",
        "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
        "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
        "custom_objects = {\n",
        "            \"learning_rate\": 0.0,\n",
        "            \"lr_schedule\": lambda _: 0.0,\n",
        "            \"clip_range\": lambda _: 0.0,\n",
        "}\n",
        "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0Y-qgPgLUf"
      },
      "source": [
        "Let's evaluate this agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAEVwK-aahfx"
      },
      "outputs": [],
      "source": [
        "eval_env = Monitor(gym.make(env_id))\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQAwLnYFPk-s"
      },
      "source": [
        "# Improve Agent\n",
        "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
        "\n",
        "Here are some ideas to get to the top of the leaderboard:\n",
        "* Train more steps\n",
        "* Try different hyperparameters for `PPO`. You can see them at https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters.\n",
        "* Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) and try another model such as DQN.\n",
        "* **Push your new trained model** on the Hub üî•\n",
        "\n",
        "Other possible environments:\n",
        "* MountainCar-v0\n",
        "* CartPole-v1\n",
        "* CarRacing-v0\n",
        "\n",
        "Check how they work at https://www.gymlibrary.dev."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDONe9iyOew6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}